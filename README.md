## StoryScraper

StoryScraper is a uv-managed Python project that downloads stories (single- or multi-chapter) from the web and emits ready-to-read bundles similar to how yt-dlp handles videos.

```
stories/
└── the-silver-leash/
    ├── download_urls.txt
    ├── html/
    ├── markdown/
    ├── out/
    ├── fetch.log
    └── transform.log
```

- `download_urls.txt`: ordered list of chapter URLs discovered during the fetch list-phase.
- `html/`: raw chapter HTML downloaded from chapter URLs (stored as `<slug>-<chapter_number:03d>.html`).
- `markdown/`: Markdown conversions generated by the auto transformer (which prefers `<main>`/`role="main">`/`<article>` roots, otherwise strips nav/header/footer chrome and falls back to the deepest heading-containing subtree or the `<body>`) using markdownify, plus a local `Makefile` for building distributable formats.
- `out/`: rendered EPUB/PDF/etc. produced by the Makefile.
- `fetch.log`: failures (if any) encountered during chapter downloads.
- `transform.log`: failures (if any) encountered while generating Markdown.

### Getting started
1. **Install uv** (https://docs.astral.sh/uv/). uv manages the virtual env, `.python-version`, and the `uv.lock`.
2. **Install dependencies**: `uv sync`.
3. **Run the CLI**: `uv run storyscraper --help`.

All source code lives under `src/`, and tests live under `tests/`. Keep new modules discoverable by adding them in `src/` so `uv run mypy src` and `uv run pytest` stay fast.

### Workflow
1. Gather the target URL(s) and an optional story name: `uv run storyscraper https://example.com/story --name "The Silver Leash"`.
2. StoryScraper will slugify the name (`the-silver-leash`) and create the directory tree shown above.
3. A site-specific downloader is used if available; otherwise the generic downloader pulls each chapter and records URLs in `download_urls.txt`.
4. The fetch-phase downloads any missing chapter HTML files into `stories/<slug>/html/<slug>-NNN.html` (use `--force-fetch` to re-download even if files exist). Failures are appended to `stories/<slug>/fetch.log`.
5. The transform-phase extracts the main content using the heuristics described above and converts it to Markdown via markdownify (with site-specific transformers like `mcstories_transformer` able to tweak the HTML before conversion), storing results as `stories/<slug>/markdown/<slug>-NNN.md` (errors go to `stories/<slug>/transform.log`).
6. The `markdown/Makefile` can then be invoked (manually or automatically) to render EPUB/other outputs into the adjacent `out/` folder.

### CLI options

```
uv run storyscraper [--name "Title"] [--slug slug] [--fetch-agent agent]
                    [--transform-agent agent] [--packaging-agent agent]
                    [--author "Author Name"] [--force-fetch] [--from-file URLFILE]
                    [--list-site-rules [json|csv|text]]
                    [--quiet | --verbose] [download-url]
```
- `--author`: specify the author name (defaults to site-specific metadata when available).
- `--force-fetch`: re-downloads every chapter even if the HTML files already exist.
- `--from-file`, `-f`: load a prebuilt list of chapter URLs (one per line), skipping list-phase URL discovery.
- `--list-site-rules`: print site rule metadata in json/csv/text and exit (defaults to json).
- `--quiet`: suppresses phase progress output (only errors/logs are emitted).
- `--verbose`: prints per-file progress within each phase (mutually exclusive with `--quiet`).
- `--cookies-from-browser`: import cookies from a supported browser profile (firefox/chrome/etc.) so downloads reuse your authenticated session.

---

Developers: the full workflow (tooling commands, architectural guardrails, and agent playbook) now lives in `AGENTS.md`.
